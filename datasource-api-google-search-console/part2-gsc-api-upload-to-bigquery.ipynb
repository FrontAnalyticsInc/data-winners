{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc0a454",
   "metadata": {
    "id": "ffc0a454"
   },
   "source": [
    "# Google Search Console GSC API Examples\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "If you are interested in your organic traffic then the Google Search Console is hands down \n",
    "one of the richest datasources.\n",
    "\n",
    "Here are example scripts for connecting to this valuable datasource.\n",
    "\n",
    "\n",
    "> SEE PART 1 for how to connect and download GSC data\n",
    "\n",
    "\n",
    "\n",
    "### About Me\n",
    "\n",
    "My name is Alton Alexander. I am a Data Science consultant turned entreprenuer building SaaS tools for SEO.\n",
    "\n",
    "Find more about my free scripts or ask me any question on twitter: @alton_lex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96450db",
   "metadata": {
    "id": "f96450db"
   },
   "source": [
    "# GSC API Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be86962",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6be86962",
    "outputId": "54e1f80e-bfa2-43f0-a70d-7f9d24e4ed53"
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import httplib2\n",
    "from apiclient import errors\n",
    "from apiclient.discovery import build\n",
    "\n",
    "import datetime\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "import google.oauth2.credentials\n",
    "import google.auth.transport.requests\n",
    "\n",
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85f66b",
   "metadata": {},
   "source": [
    "## 0) Setup service account\n",
    "\n",
    "In Google Cloud Console you need to create the following resources:\n",
    "\n",
    "1. A project\n",
    "\n",
    "2. Enable Access to the GSC API\n",
    "\n",
    "3. A service account (make note of the Email for the service account)\n",
    "\n",
    "4. OPTIONAL Add rolls to the service account so that it can access additional resources (ie. BigQuery)\n",
    "\n",
    "5. Save the Key as JSON for connecting from python\n",
    "\n",
    "\n",
    "In Google Search Console:\n",
    "\n",
    "1. add the service account email as a user to each of the properties you want it to access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db8be5",
   "metadata": {
    "id": "f2db8be5"
   },
   "source": [
    "## 1) Connect with Service Account JSON key\n",
    "\n",
    "Every request to the GSC API must have user authentication.\n",
    "\n",
    "We use the service account JSON Key we get credentials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2tqk1Fsq-ZZf",
   "metadata": {
    "id": "2tqk1Fsq-ZZf"
   },
   "outputs": [],
   "source": [
    "# local path to the service account key\n",
    "# service account's email must be added to one or more properties in GSC\n",
    "\n",
    "#SERVICE_ACCOUNT_FILE = \"../gcp-keys/website-analytics-161019-16456165cddc.json\"\n",
    "SERVICE_ACCOUNT_FILE = \"./service-account-key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hsV0trhH-VAc",
   "metadata": {
    "id": "hsV0trhH-VAc"
   },
   "outputs": [],
   "source": [
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/webmasters',\n",
    "    'https://www.googleapis.com/auth/bigquery'\n",
    "]\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6UzXC2B-7nN",
   "metadata": {
    "id": "f6UzXC2B-7nN"
   },
   "outputs": [],
   "source": [
    "service = build(\n",
    "    'webmasters',\n",
    "    'v3',\n",
    "    credentials=credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba56af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test by pulling a list of sites\n",
    "# reminder to add the service account email to every GSC property as a user\n",
    "\n",
    "\n",
    "response = service.sites().list().execute()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa89e6",
   "metadata": {
    "id": "34aa89e6"
   },
   "source": [
    "## 1) Pull a list of pages\n",
    "\n",
    "This pulls a list of all the pages that are showing on Google Search in the last 30 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395fa428",
   "metadata": {
    "id": "395fa428"
   },
   "outputs": [],
   "source": [
    "# define the domain that we are using\n",
    "\n",
    "# hardcode the domain for the property\n",
    "#site = \"frontanalytics.com\"\n",
    "\n",
    "# or select the first property from the response above\n",
    "site = response['siteEntry'][0]['siteUrl'].split(\":\")[1]\n",
    "site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cdc22",
   "metadata": {
    "id": "9f8cdc22"
   },
   "outputs": [],
   "source": [
    "# set date for the last 30 days\n",
    "today = datetime.datetime.today()\n",
    "startDate = today - datetime.timedelta(days=32)\n",
    "endDate = today - datetime.timedelta(days=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64b56d",
   "metadata": {
    "id": "2d64b56d"
   },
   "outputs": [],
   "source": [
    "# by page\n",
    "\n",
    "# recent\n",
    "# Set the dates in this format \"2022-12-01\"\n",
    "data = {\n",
    "  \"startDate\": startDate.strftime(\"%Y-%m-%d\"),\n",
    "  \"endDate\": endDate.strftime(\"%Y-%m-%d\"),\n",
    "  \"dimensions\": \"page\"\n",
    "}\n",
    "res = requests.post(\"https://www.googleapis.com/webmasters/v3/sites/\"+\"sc-domain:\"+site+\"/searchAnalytics/query?access_token=\"+credentials.token, json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68effdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b51988",
   "metadata": {
    "id": "88b51988",
    "outputId": "46cb60ec-52e8-4a77-9d89-827a6eb47d97"
   },
   "outputs": [],
   "source": [
    "\n",
    "if str(res.status_code) != \"200\":\n",
    "    print(\"error with the api response. verify credentials are active.\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    # continue\n",
    "    j = json.loads(res.text).get('rows',[])\n",
    "    \n",
    "    for i in range(len(j)):\n",
    "        j[i]['url'] = j[i]['keys'][0]\n",
    "\n",
    "    df_pages = pd.DataFrame(j)\n",
    "\n",
    "    df_pages = df_pages.sort_values('impressions',ascending=False)\n",
    "\n",
    "df_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b72651",
   "metadata": {
    "id": "43b72651"
   },
   "source": [
    "## 2) Get all KWs for each page\n",
    "\n",
    "This pulls every query used for each page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82aae2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e82aae2a",
    "outputId": "046e216d-5c01-4639-88ce-d41fdccb53fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set date for the previous 1 days (day before yesterday)\n",
    "updated_at = datetime.datetime.today()\n",
    "today = updated_at.date()\n",
    "startDate = today - datetime.timedelta(days=4)\n",
    "endDate = today - datetime.timedelta(days=4)\n",
    "\n",
    "# choose the dimensions you want in your dataset\n",
    "\n",
    "#dim_list = [\"query\",\"device\",\"country\"]\n",
    "dim_list = [\"query\",\"device\",\"country\"] # Add device and country if desired\n",
    "\n",
    "# for testing\n",
    "max_n_pages = 3\n",
    "\n",
    "df_all_queries = pd.DataFrame()\n",
    "\n",
    "# Get all the queries\n",
    "for index, row in df_pages[0:max_n_pages].iterrows():\n",
    "    \n",
    "    # get the url of this page\n",
    "    page_url = row['url']\n",
    "\n",
    "    # recent\n",
    "    data = {\n",
    "      \"startDate\": startDate.strftime(\"%Y-%m-%d\"),\n",
    "      \"endDate\": endDate.strftime(\"%Y-%m-%d\"),\n",
    "      \"dimensions\": dim_list,\n",
    "      \"dimensionFilterGroups\": [\n",
    "        {\n",
    "          \"groupType\": \"and\",\n",
    "          \"filters\": [\n",
    "            {\n",
    "              \"dimension\": \"page\",\n",
    "              \"operator\": \"contains\",\n",
    "              \"expression\": page_url\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    res = requests.post(\"https://www.googleapis.com/webmasters/v3/sites/\"+\"sc-domain:\"+site+\"/searchAnalytics/query?access_token=\"+credentials.token, json=data)\n",
    "\n",
    "    if str(res.status_code) != \"200\":\n",
    "        print(\"error with the api response. verify credentials are active.\")\n",
    "        continue\n",
    "        \n",
    "    # convert the response to a data frame\n",
    "    j = json.loads(res.text).get('rows',[])\n",
    "\n",
    "    if(len(j)):\n",
    "        df_queries = pd.DataFrame(j)\n",
    "        df_queries['url'] = page_url\n",
    "        df_queries['property'] = site\n",
    "        df_queries['start_date'] = startDate\n",
    "        df_queries['updated_at'] = updated_at\n",
    "\n",
    "        # By default the keys/dimensions are in a single column, let's split them out into separate columns.\n",
    "        new_cols = df_queries['keys'].astype(str).str.replace(\"[\",\"\").str.replace(\"]\",\"\")\n",
    "        new_cols = new_cols.str.split(pat=',',expand=True,n=2)\n",
    "\n",
    "        # Give the columsn sensible names\n",
    "        new_cols.columns = dim_list\n",
    "\n",
    "        # Bring back a key from the intial dataframe so we can join\n",
    "        new_cols['key'] = df_queries['keys']\n",
    "\n",
    "        # Get rid of quotation marks or set to None\n",
    "        new_cols['query'] = new_cols.apply(lambda row : row['query'].replace(\"'\",\"\").lower(), axis=1)\n",
    "        \n",
    "        if 'device' in new_cols:\n",
    "            new_cols['device'] = new_cols.apply(lambda row : row['device'].replace(\"'\",\"\").lower(), axis=1)\n",
    "        else:\n",
    "            new_cols['device'] = None\n",
    "        if 'country' in new_cols:\n",
    "            new_cols['country'] = new_cols.apply(lambda row : row['country'].replace(\"'\",\"\").lower(), axis=1)\n",
    "        else:\n",
    "            new_cols['country'] = None\n",
    "\n",
    "        # Join in the new clean columns to our intiial data\n",
    "        df_queries = pd.concat([df_queries, new_cols], axis=1, join='inner')\n",
    "        \n",
    "        # Drop the key column and ctr\n",
    "        df_queries = df_queries.drop([\"key\",\"keys\", \"ctr\"],axis=1)\n",
    "\n",
    "        # save all the queries for this page with all other pages\n",
    "        df_all_queries = pd.concat([df_all_queries, df_queries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842386cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "842386cc",
    "outputId": "4fc484a5-afc4-4fe7-81df-577176594c1e"
   },
   "outputs": [],
   "source": [
    "# Now you can save df_all_queries for additional analysis\n",
    "df_all_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e1e10",
   "metadata": {},
   "source": [
    "# Configure BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30f54c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud\n",
    "!pip install --upgrade google-cloud-bigquery\n",
    "!pip install --upgrade google-cloud-storage\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# establish a BigQuery client\n",
    "client_bq = bigquery.Client.from_service_account_json(SERVICE_ACCOUNT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e46dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to big query and list all datasets\n",
    "\n",
    "datasets = client_bq.list_datasets()\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset.dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the project name from the service account\n",
    "\n",
    "BQ_PROJECT_NAME = client_bq.project\n",
    "BQ_PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27e2ad",
   "metadata": {},
   "source": [
    "## Create the BQ Dataset and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bc83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset and table if they don't exist yet\n",
    "\n",
    "BQ_DATASET_NAME = 'data_winners_dataset'\n",
    "BQ_TABLE_NAME = 'gsc_daily_table'\n",
    "\n",
    "\n",
    "# create the dataset\n",
    "try:\n",
    "    # Construct a full Dataset object to send to the API.\n",
    "    dataset = bigquery.Dataset(\"{}.{}\".format(client_bq.project, BQ_DATASET_NAME))\n",
    "\n",
    "    # TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    # Send the dataset to the API for creation, with an explicit timeout.\n",
    "    # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "    # exists within the project.\n",
    "    dataset = client_bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "    print(\"Created dataset {}.{}\".format(client_bq.project, dataset.dataset_id))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# create the table\n",
    "# https://cloud.google.com/bigquery/docs/samples/bigquery-create-table \n",
    "try:\n",
    "    # TODO(dev): Change table_id to the full name of the table you want to create.\n",
    "    #table_id = \"your-project.your_dataset.your_table_name\"\n",
    "    table_id = \"{}.{}.{}\".format(client_bq.project, BQ_DATASET_NAME, BQ_TABLE_NAME)\n",
    "    \n",
    "    #SCHEMA must match datatypes to be loaded\n",
    "    # see. https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"start_date\", \"DATE\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"url\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"impressions\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"position\", \"FLOAT64\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"query\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"device\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"country\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"clicks\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"updated_at\", \"DATETIME\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"property\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    ]\n",
    "\n",
    "    table_description = \"Stores GSC data every day. Auto generated via data-winners. For details see https://github.com/FrontAnalyticsInc/data-winners/tree/main/datasource-api-google-search-console\"\n",
    "    \n",
    "    table = bigquery.Table(table_id, schema=schema)\n",
    "    table.description = table_description\n",
    "    table = client_bq.create_table(table)  # API request\n",
    "\n",
    "    print(f\"Created {table_id}.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0c90f",
   "metadata": {},
   "source": [
    "# Test Uploading to BQ\n",
    "\n",
    "Make sure your service account has a role that permits access to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C5ArtLUDE7aA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5ArtLUDE7aA",
    "outputId": "63ab341e-22db-48c8-b566-ff337e71f446"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set the destination table\n",
    "table_id = '{}.{}.{}'.format(BQ_PROJECT_NAME, BQ_DATASET_NAME, BQ_TABLE_NAME)\n",
    "\n",
    "\n",
    "# create a load job config\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    # Specify a (partial) schema. All columns are always written to the\n",
    "    # table. The schema is used to assist in data type definitions.\n",
    "    schema=[\n",
    "        # Specify the type of columns whose type cannot be auto-detected. For\n",
    "        # example the \"title\" column uses pandas dtype \"object\", so its\n",
    "        # data type is ambiguous.\n",
    "        #bigquery.SchemaField(\"start_date\", bigquery.enums.SqlTypeNames.DATE),\n",
    "        #bigquery.SchemaField(\"updated_at\", bigquery.enums.SqlTypeNames.DATETIME),\n",
    "        # Indexes are written if included in the schema by name.\n",
    "        #bigquery.SchemaField(\"wikidata_id\", bigquery.enums.SqlTypeNames.STRING),\n",
    "    ],\n",
    "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "    # disposition it replaces the table with the loaded data.\n",
    "    write_disposition=\"WRITE_APPEND\",\n",
    ")\n",
    "\n",
    "job = client_bq.load_table_from_dataframe(\n",
    "    df_all_queries, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d70d8e",
   "metadata": {},
   "source": [
    "# Preparing for Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01619d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas-gbq\n",
    "!pip install db-dtypes\n",
    "import dateutil\n",
    "import pandas_gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9bd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query bigquery to determine what has already been loaded?\n",
    "# Update the in-memory credentials cache (added in pandas-gbq 0.7.0).\n",
    "pandas_gbq.context.credentials = credentials\n",
    "pandas_gbq.context.project = BQ_PROJECT_NAME\n",
    "\n",
    "\n",
    "def query_table(start_date, PROJECT_ID, DATASET, TABLE):\n",
    "    QUERY = \"SELECT * FROM {dataset}.{table} WHERE start_date = '{d}'\"\n",
    "    \n",
    "    query = QUERY.format(dataset=DATASET, table=TABLE, d=start_date)\n",
    "    result = pd.read_gbq(query, PROJECT_ID, dialect='standard')\n",
    "\n",
    "    result['already_loaded'] = True\n",
    "    return result\n",
    "\n",
    "\n",
    "# get all the records already loaded within the last n days\n",
    "already_loaded = query_table(startDate, BQ_PROJECT_NAME, BQ_DATASET_NAME, BQ_TABLE_NAME)\n",
    "\n",
    "already_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rows that aren't loaded yet\n",
    "join_cols = ['start_date','property','url','device','country']\n",
    "not_loaded = df_all_queries.merge(already_loaded[join_cols+['already_loaded']], how=\"left\", on=join_cols)\n",
    "not_loaded = not_loaded[not_loaded['already_loaded'].isna()]\n",
    "cols = list(df_all_queries.columns)\n",
    "not_loaded = not_loaded[cols]\n",
    "not_loaded"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
